{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebe948db",
   "metadata": {},
   "source": [
    "1. TDM에서 특정 문서에서만 자주 사용된 단어에 대해서는 바이럴 마케팅 단어로 판단한다. (업체 정보에 대한 과한 어필) #\n",
    "     ->  추후 머신러닝을 통해 해당 단어들을 학습하고, 바이럴 마케팅 구분 문자인지를 학습시킨다.\n",
    "2. 고료에 관한 언급이 있는 경우 바이럴 마케팅 글로 판단한다. #\n",
    "3. Co-occurrence에서 일반적으로 사용되지 않는 문자 조합이 있는 경우, 어색한 문장으로 판단하여 바이럴 마케팅 데이터로 의심한다.\n",
    "    -> 추후 머신러닝을 도입하여 일반적인 문자 조합에 대한 학습\n",
    "4. TDM에서 바이럴 마케팅 글에 사용된 문자 중 1 및 2에 대한 문자를 머신러닝이 학습하고, 중요도를 부여하여 추후 다른 글을 분석할 때, 바이럴 마케팅 글 여부를 판단하는 척도로 사용한다.\n",
    "5. 다른 글과 비교할 때 유사한 문장 수를 분석하여 바이럴 마케팅 글 구분에 대한 척도로 사용한다. #\n",
    "6. 일반적으로 사용하지 않은 단어를 사용한 경우 바이럴 마케팅 문구로 판단. #\n",
    "    -> 추후 머신러닝을 통해 가중치 부여 및 판단 척도로 사용\n",
    "7. Co-occurrence matrix 분석 결과와 머신러닝을 통해 바이럴 마케팅 글에서 사용한 문자 조합(문장)에 바이럴 마케팅 데이터로써의 가중치를 부여한다.\n",
    "    -> 추후 새로운 데이터가 들어왔을 때, 바이럴 마케팅 글을 판별하는데 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df27ce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구현 시간 체크\n",
    "import time\n",
    "# 시작시간 체크\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b218c72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://blog.naver.com/sjewelryshop/2223099011...</td>\n",
       "      <td>\\n\\n\\n\\n\\n​안녕하세요\\n 맛집 찾아주는 먹스타입니다\\n 오늘은 며칠전에 친...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>298.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://blog.naver.com/happylife_mi/2221038191...</td>\n",
       "      <td>\\n\\n\\n\\n\\n얼마전에는 친구 만나러 인천에 갔다가새로 오픈했다는 가정동 맛집을...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://blog.naver.com/lemonade0601/2223512193...</td>\n",
       "      <td>\\n\\n\\n\\n\\n​스시를 정말 좋아하는 힝구와 쯀리는 가정동 루원시티에서 스시 맛...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://blog.naver.com/pocck20/222276181608/가정...</td>\n",
       "      <td>\\n\\n\\n\\n\\n친한 지인 중 한 명이 청라에 사는데 간만에 지인 얼굴도 보러 갈...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://blog.naver.com/newjinha/222327524869/인...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n며칠 전 고등학교 동창에게...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>https://blog.naver.com/elqlahs2/222074909812/순...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n순천에서는 어느 식당을 가...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>https://blog.naver.com/ggul33/222158547915/#칠곡...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>https://blog.naver.com/baiksange/221631319690/...</td>\n",
       "      <td>\\n\\n\\n\\n호동이식당 처음 가 봤는데 좋은걸요♡진주에서 강소농회원들과 함께 참석...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4001</th>\n",
       "      <td>https://blog.naver.com/yjoshjh/222274845825/[2...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n2019\\n3\\n12\\n\\n2년 전 오늘\\n\\n\\n...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4002</th>\n",
       "      <td>https://blog.naver.com/ruinrose8/221415101575/...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4003 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    url                                               text Unnamed: 2 Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6\n",
       "0     https://blog.naver.com/sjewelryshop/2223099011...  \\n\\n\\n\\n\\n​안녕하세요\\n 맛집 찾아주는 먹스타입니다\\n 오늘은 며칠전에 친...        3.0      298.0        0.0       97.0        1.0\n",
       "1     https://blog.naver.com/happylife_mi/2221038191...  \\n\\n\\n\\n\\n얼마전에는 친구 만나러 인천에 갔다가새로 오픈했다는 가정동 맛집을...        1.0      291.0        0.0       82.0        8.0\n",
       "2     https://blog.naver.com/lemonade0601/2223512193...  \\n\\n\\n\\n\\n​스시를 정말 좋아하는 힝구와 쯀리는 가정동 루원시티에서 스시 맛...        2.0      184.0        0.0       58.0        2.0\n",
       "3     https://blog.naver.com/pocck20/222276181608/가정...  \\n\\n\\n\\n\\n친한 지인 중 한 명이 청라에 사는데 간만에 지인 얼굴도 보러 갈...        2.0      214.0        0.0       54.0        5.0\n",
       "4     https://blog.naver.com/newjinha/222327524869/인...  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n며칠 전 고등학교 동창에게...        1.0      140.0        0.0       38.0        4.0\n",
       "...                                                 ...                                                ...        ...        ...        ...        ...        ...\n",
       "3998  https://blog.naver.com/elqlahs2/222074909812/순...  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n순천에서는 어느 식당을 가...                                                       \n",
       "3999  https://blog.naver.com/ggul33/222158547915/#칠곡...                                               None                                                       \n",
       "4000  https://blog.naver.com/baiksange/221631319690/...  \\n\\n\\n\\n호동이식당 처음 가 봤는데 좋은걸요♡진주에서 강소농회원들과 함께 참석...                                                       \n",
       "4001  https://blog.naver.com/yjoshjh/222274845825/[2...  \\n\\n\\n\\n\\n\\n\\n\\n\\n2019\\n3\\n12\\n\\n2년 전 오늘\\n\\n\\n...                                                       \n",
       "4002  https://blog.naver.com/ruinrose8/221415101575/...                                               None                                                       \n",
       "\n",
       "[4003 rows x 7 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('display.max_rows', 999) # pd.options.display.max_rows = 999\n",
    "pd.set_option('display.max_columns', 999) # pd.options.display.max_columns = 999\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# 한글폰트 적용\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "\n",
    "# read text data set\n",
    "text_set = pd.read_excel('social_data.xlsx')\n",
    "text_set = text_set.fillna(\"\")\n",
    "text_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a447f9e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4003, 7)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "751f12eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url           string\n",
       "text          string\n",
       "Unnamed: 2    object\n",
       "Unnamed: 3    object\n",
       "Unnamed: 4    object\n",
       "Unnamed: 5    object\n",
       "Unnamed: 6    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_set = text_set.astype({'text':'string', 'url':'string'})\n",
    "text_set.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e983d35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_tmi_words_value: [3 1 2 2 1]\n",
      "own_words_value [298 291 184 214 140]\n",
      "commissional_words_value: [0 0 0 0 0]\n",
      "similar_sentence_value_all: [97 82 58 54 38]\n",
      "similar_sentence_value_bes:  [1 8 2 5 4]\n",
      "Analysis position: 0 -> 5\n",
      "\n",
      "\n",
      "use_tmi_words_value: [8 7 3 0 8]\n",
      "own_words_value [207 261 132 164 320]\n",
      "commissional_words_value: [0 0 0 0 0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-9fb108ace497>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    208\u001b[0m                 \u001b[0mcomp_to_line_sub_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcomp\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mline_len\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcomp_len\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mline_to_comp_sub_len\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcomp_to_line_sub_len\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mline_len\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcomp_len\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m                         \u001b[0msimilar_sentence_value_all\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimilar_sentence_value_all\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "import itertools\n",
    "\n",
    "import copy\n",
    "import openpyxl\n",
    "\n",
    "\n",
    "# co-occurrence matrix를 형성하는 함수\n",
    "def generate_co_occurrence_matrix(corpus):\n",
    "    vocab = set(corpus)\n",
    "    vocab = list(vocab)\n",
    "    vocab_index = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "    bi_grams = list(bigrams(corpus))\n",
    "    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n",
    "\n",
    "    co_occurrence_matrix = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "    for bigram in bigram_freq:\n",
    "        current = bigram[0][1]\n",
    "        previous = bigram[0][0]\n",
    "        count = bigram[1]\n",
    "        pos_current = vocab_index[current]\n",
    "        pos_previous = vocab_index[previous]\n",
    "        co_occurrence_matrix[pos_current][pos_previous] = count\n",
    "\n",
    "    co_occurrence_matrix = np.matrix(co_occurrence_matrix, dtype=np.int32)\n",
    "    return co_occurrence_matrix, vocab_index\n",
    "\n",
    "\n",
    "wb = openpyxl.load_workbook('submission.xlsx')\n",
    "sheet = wb.active\n",
    "    \n",
    "sheet.cell(row=1, column=1).value = \"과도한 정보 표기\"\n",
    "sheet.cell(row=1, column=2).value = \"고유 단어 사용\"\n",
    "sheet.cell(row=1, column=3).value = \"고료 표기\"\n",
    "sheet.cell(row=1, column=4).value = \"유사한 문장 사용(본문 포함)\"\n",
    "sheet.cell(row=1, column=5).value = \"유사한 문장 사용(본문 미포함)\"\n",
    "\n",
    "wb.save(\"submission.xlsx\")\n",
    "\n",
    "analysis_count = 0\n",
    "while analysis_count < len(text_set):\n",
    "    if analysis_count+5 < len(text_set)-1:\n",
    "        text_sub_set = text_set[analysis_count : analysis_count+5]\n",
    "    else:\n",
    "        text_sub_set = text_set[analysis_count : len(text_set)-1]\n",
    "        \n",
    "    \n",
    "    # matrix(DataFrame)에서 text데이터를 가져온다.\n",
    "    text_list = text_sub_set['text']\n",
    "    \n",
    "    # 한국어 조사 정리.\n",
    "    # 한국어 문법론 대가 이익섭 서울대 명예교수님께서 쓰신 '한국어문법'참고.\n",
    "    # 자연어처리 분야에서 한국어 조사는 분석의 까다로움으로 인해 전처리 때 아예 제거되는 불용어(stop_words)로 취급됨. \n",
    "    stop_words = [\"은\", \"는\", \"을\", \"를\", \"이\", \"가\", \"의\", \"에\", \"로\", \"으로\", \"과\", \"와\", \"도\", \"에서\", \"만\"\n",
    "                 , \"이나\", \"나\", \"까지\", \"부터\", \"에게\", \"보다\", \"께\", \"처럼\", \"이라도\", \"라도\", \"으로서\", \"로서\"\n",
    "                 , \"조차\", \"만큼\", \"같이\", \"마저\", \"이나마\", \"나마\", \"한테\", \"더러\", \"에게서\", \"한테서\", \"께서\"\n",
    "                 , \"이야\", \"이라야\"]\n",
    "\n",
    "    commissional_words = [\"협찬\", \"고료\", \"광고\", \"후원\", \"원고\"]\n",
    "\n",
    "    # Open Korea Text를 사용한 명사 추출 모듈 형성 - tokenizer진행\n",
    "    # 본래 CountVectorizer는 토크나이징과 벡터화를 동시해 해주나, 이는 한국어를 대상으로하지는 않음.\n",
    "    # 따라서 Okt모듈을 통해 토크나이징을 먼저 한 후 CountVectorizer작업 필요\n",
    "    okt=Okt()\n",
    "    text_token_set = list()\n",
    "\n",
    "    count = 0\n",
    "    for text in text_list.tolist():\n",
    "        text_token_set.append(okt.nouns(text))\n",
    "\n",
    "    # 시스템이 2층, 2층에, 2층으로 와 같은 글들을 각기 다른 하나의 명사 단어로 판단할수 있기에 토크나이징을 처리함\n",
    "\n",
    "\n",
    "    # 출력결과 NxM: N개의 데이터에서 M개의 데이터를 뽑아냄: 벡터화\n",
    "    # 결과적으로 각 문서에 어떤 단어가 몇번 등장했는지를 파악할 수 있음\n",
    "    try:\n",
    "        cv = CountVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
    "        tdm = cv.fit_transform(text_token_set)\n",
    "    except ValueError:\n",
    "        dummy_value = np.zeros(shape=(len(text_list),), dtype=np.int64)\n",
    "        \n",
    "        wb = openpyxl.load_workbook('submission.xlsx')\n",
    "        sheet = wb.active\n",
    "\n",
    "        for row in range(1, 6):\n",
    "            sheet.cell(row=analysis_count+row+1, column=1).value = dummy_value[row-1]\n",
    "            sheet.cell(row=analysis_count+row+1, column=2).value = dummy_value[row-1]\n",
    "            sheet.cell(row=analysis_count+row+1, column=3).value = dummy_value[row-1]\n",
    "            sheet.cell(row=analysis_count+row+1, column=4).value = dummy_value[row-1]\n",
    "            sheet.cell(row=analysis_count+row+1, column=5).value = dummy_value[row-1]\n",
    "\n",
    "        wb.save(\"submission.xlsx\")\n",
    "\n",
    "        analysis_count = analysis_count+5\n",
    "\n",
    "\n",
    "    # TDM 출력\n",
    "    # TDM 대규모 데이터에서 대부분의 값은 0으로 나타낼 것임.\n",
    "    # 이유는 하나의 텍스트에 2000개 종류의 단어를 사용한다 해도, 전체 단어 셋은 몇 만개는 될 것이기에\n",
    "    # 메모리 부족 문제를 초래하기 쉽기 때문에, CountVectorizer는 희소행렬을 사용하기 때문.\n",
    "    # 다음 출력 결과는 희소행렬임\n",
    "    tdm_dataframe = pd.DataFrame(tdm.toarray())\n",
    "\n",
    "    # TDM 분석: 너무 독자적인 단어 사용, 너무 TMI적 단어 사용 감지 (평균 * 5회 보다 사용수가 많거나 같고, 5회 이상 사용된 단어)\n",
    "    use_tmi_words_value = np.zeros(shape=(len(text_list),), dtype=np.int64)\n",
    "    # TDM 분석: 모든 독자적인 단어에 대한 수치 (혼자 사용된 단어가 10개중 1개 미만 일 경우)\n",
    "    own_words_value = np.zeros(shape=(len(text_list),), dtype=np.int64)\n",
    "\n",
    "    for col in range(0, tdm_dataframe.shape[1]):\n",
    "        avg = sum(tdm_dataframe[col], 0.0) / len(tdm_dataframe[col]) # avg * 5 < 특정한 문서에서만 TMI적으로 사용된 단어 사용 빈도\n",
    "\n",
    "        for row in range(0, len(tdm_dataframe[col])):\n",
    "            if tdm_dataframe[col][row] >= (avg * 5) and tdm_dataframe[col][row] >= 5:\n",
    "                use_tmi_words_value[row] = use_tmi_words_value[row] + 1\n",
    "\n",
    "    print(\"use_tmi_words_value:\", use_tmi_words_value)\n",
    "\n",
    "\n",
    "    # TDM 분석: 모든 독자적인 단어에 대한 수치 (혼자 사용된 단어가 10개중 1개 미만 일 경우)\n",
    "    own_words_value = np.zeros(shape=(len(text_list),), dtype=np.int64)\n",
    "\n",
    "    for col in range(0, tdm_dataframe.shape[1]):\n",
    "        used_value = 0\n",
    "        for row in tdm_dataframe[col]:\n",
    "            if row != 0:\n",
    "                used_value += 1\n",
    "        for row in range(0, len(tdm_dataframe[col])):\n",
    "            if(len(text_list) == 0):\n",
    "                continue\n",
    "            \n",
    "            if len(text_list) >= 20:\n",
    "                if (used_value / 20) <= 1 and tdm_dataframe[col][row] != 0: \n",
    "                    own_words_value[row] = own_words_value[row] + 1\n",
    "\n",
    "            else:\n",
    "                if(used_value / len(text_list)) <= 1 and tdm_dataframe[col][row] != 0:\n",
    "                    own_words_value[row] = own_words_value[row] + 1\n",
    "\n",
    "\n",
    "    print(\"own_words_value\", own_words_value)\n",
    "\n",
    "\n",
    "    commissional_words_value = np.zeros(shape=(len(text_list),), dtype=np.int64)\n",
    "\n",
    "    for index in range(0, len(text_list)):\n",
    "        for word in commissional_words:\n",
    "            if(len(text_list) == 0):\n",
    "                continue\n",
    "            if(text_list.tolist()[index].find(word) != -1):\n",
    "                commissional_words_value[index] = 1\n",
    "\n",
    "    print(\"commissional_words_value:\", commissional_words_value)\n",
    "\n",
    "\n",
    "    # # Co-occurrence matrix 형성\n",
    "\n",
    "\n",
    "    co_occurrence_matrix_list = list()\n",
    "\n",
    "    for text in text_list:\n",
    "        tmp = text.split(\"\\n\")\n",
    "\n",
    "        text_data = [okt.nouns(line) for line in tmp]\n",
    "        text_data = list(itertools.chain.from_iterable(text_data))\n",
    "\n",
    "        matrix, vocab_index = generate_co_occurrence_matrix(text_data)\n",
    "        matrix_dataframe = pd.DataFrame(matrix, index=vocab_index, columns=vocab_index)\n",
    "        co_occurrence_matrix_list.append(matrix_dataframe)\n",
    "        \n",
    "\n",
    "    # Co-occurrence matrix 분석: 너무 독자적인 단어 사용, 너무 TMI적 단어 사용 감지\n",
    "    awkward_sentence_value = np.zeros(shape=(len(text_list),), dtype=np.int64)\n",
    "\n",
    "\n",
    "    # # 비슷한 문장 판독\n",
    "\n",
    "    # 1. 모든 문장에 대해 BoW적용\n",
    "    # 2. 모든 text 대이터에 대해 대해 유사글 수치 부여 (갱신)\n",
    "    # 3. 모든 text 데이터에 대해 유사글 수치 평균 계산\n",
    "\n",
    "\n",
    "    # text데이터 별 비슷한 문장 수 리스트\n",
    "    similar_sentence_value_all = np.zeros(shape=(len(text_list),), dtype=np.int64)\n",
    "    similar_sentence_value_bes = np.zeros(shape=(len(text_list),), dtype=np.int64)\n",
    "\n",
    "    # text_list토큰화\n",
    "    text_data_token_set = list()\n",
    "    for text in text_list:\n",
    "        lines = text.split('\\n')\n",
    "        lines_token_set = [okt.nouns(line) for line in lines]\n",
    "\n",
    "        text_data_token_set.append(lines_token_set)\n",
    "\n",
    "    comp_list = list(itertools.chain.from_iterable(text_data_token_set))\n",
    "\n",
    "    for i in range(0, len(text_list)):\n",
    "        for line in text_data_token_set[i]:\n",
    "            for comp in comp_list:\n",
    "                line_len = len(line)\n",
    "                comp_len = len(comp)\n",
    "                line_to_comp_sub_len = len([x for x in line if x not in comp])\n",
    "                comp_to_line_sub_len = len([x for x in comp if x not in line])\n",
    "\n",
    "                if line_len + comp_len != 0:\n",
    "                    if (line_to_comp_sub_len + comp_to_line_sub_len) / (line_len + comp_len) < 0.5:\n",
    "                        similar_sentence_value_all[i] = similar_sentence_value_all[i] + 1\n",
    "\n",
    "    for i in range(0, len(text_list)):\n",
    "        for line in text_data_token_set[i]:\n",
    "\n",
    "            explore_dest = copy.deepcopy(text_data_token_set)\n",
    "            del explore_dest[i]\n",
    "            for comp in list(itertools.chain.from_iterable(explore_dest)):\n",
    "                line_len = len(line)\n",
    "                comp_len = len(comp)\n",
    "                line_to_comp_sub_len = len([x for x in line if x not in comp])\n",
    "                comp_to_line_sub_len = len([x for x in comp if x not in line])\n",
    "\n",
    "                if line_len + comp_len != 0:\n",
    "                    if (line_to_comp_sub_len + comp_to_line_sub_len) / (line_len + comp_len) < 0.5:\n",
    "                        similar_sentence_value_bes[i] = similar_sentence_value_bes[i] + 1\n",
    "\n",
    "    print(\"similar_sentence_value_all:\", similar_sentence_value_all)\n",
    "    print(\"similar_sentence_value_bes: \", similar_sentence_value_bes)\n",
    "    print(\"Analysis position:\", analysis_count, \"->\", analysis_count+5)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    \n",
    "    wb = openpyxl.load_workbook('submission.xlsx')\n",
    "    sheet = wb.active\n",
    "\n",
    "    for row in range(1, 6):\n",
    "        if(len(use_tmi_words_value) < row):\n",
    "            break\n",
    "        sheet.cell(row=analysis_count+row+1, column=1).value = use_tmi_words_value[row-1]\n",
    "        sheet.cell(row=analysis_count+row+1, column=2).value = own_words_value[row-1]\n",
    "        sheet.cell(row=analysis_count+row+1, column=3).value = commissional_words_value[row-1]\n",
    "        sheet.cell(row=analysis_count+row+1, column=4).value = similar_sentence_value_all[row-1]\n",
    "        sheet.cell(row=analysis_count+row+1, column=5).value = similar_sentence_value_bes[row-1]\n",
    "        \n",
    "    wb.save(\"submission.xlsx\")\n",
    "    \n",
    "    analysis_count = analysis_count+5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464ddf71",
   "metadata": {},
   "source": [
    "# 결과 기록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a56ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 종료 시간 확인\n",
    "end = time.time()\n",
    "# 모델 구동 시간 계산\n",
    "print(f\"모델구동시간 : {end-start} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5c2365",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
