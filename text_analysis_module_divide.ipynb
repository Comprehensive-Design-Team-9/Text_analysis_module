{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebe948db",
   "metadata": {},
   "source": [
    "1. TDM에서 특정 문서에서만 자주 사용된 단어에 대해서는 바이럴 마케팅 단어로 판단한다. (업체 정보에 대한 과한 어필) #\n",
    "     ->  추후 머신러닝을 통해 해당 단어들을 학습하고, 바이럴 마케팅 구분 문자인지를 학습시킨다.\n",
    "2. 고료에 관한 언급이 있는 경우 바이럴 마케팅 글로 판단한다. #\n",
    "3. Co-occurrence에서 일반적으로 사용되지 않는 문자 조합이 있는 경우, 어색한 문장으로 판단하여 바이럴 마케팅 데이터로 의심한다.\n",
    "    -> 추후 머신러닝을 도입하여 일반적인 문자 조합에 대한 학습\n",
    "4. TDM에서 바이럴 마케팅 글에 사용된 문자 중 1 및 2에 대한 문자를 머신러닝이 학습하고, 중요도를 부여하여 추후 다른 글을 분석할 때, 바이럴 마케팅 글 여부를 판단하는 척도로 사용한다.\n",
    "5. 다른 글과 비교할 때 유사한 문장 수를 분석하여 바이럴 마케팅 글 구분에 대한 척도로 사용한다. #\n",
    "6. 일반적으로 사용하지 않은 단어를 사용한 경우 바이럴 마케팅 문구로 판단. #\n",
    "    -> 추후 머신러닝을 통해 가중치 부여 및 판단 척도로 사용\n",
    "7. Co-occurrence matrix 분석 결과와 머신러닝을 통해 바이럴 마케팅 글에서 사용한 문자 조합(문장)에 바이럴 마케팅 데이터로써의 가중치를 부여한다.\n",
    "    -> 추후 새로운 데이터가 들어왔을 때, 바이럴 마케팅 글을 판별하는데 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b218c72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('display.max_rows', 999) # pd.options.display.max_rows = 999\n",
    "pd.set_option('display.max_columns', 999) # pd.options.display.max_columns = 999\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# 한글폰트 적용\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "230485a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_url_title_text_20210610072118.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://blog.naver.com/fa1772/222377394130</td>\n",
       "      <td>취향저격 충남대 맛집 리스트</td>\n",
       "      <td>\\n\\n\\n\\n\\n최근 동창 친구와 약속이 있어서 충남대 맛집에 다녀왔는데요품질 좋...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://blog.naver.com/jhw7891/222038502625</td>\n",
       "      <td>혼밥하기 좋은 충남대 식당 5 / 충남대 맛집/ 충남대 혼밥/ 충대맛집</td>\n",
       "      <td>\\n\\n\\n\\n\\n길고 길었던 학기가 끝나고 이제 계절마저 끝으로 다가오고있다. ​...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://blog.naver.com/apfjtzzz/222354696565</td>\n",
       "      <td>충남대 맛집 :: 삼각김밥의 클라쓰를 바꾼 강다짐</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n​남편은 예전부터 편의점 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://blog.naver.com/jmljw/222350051816</td>\n",
       "      <td>대전 마라탕 초보입에도 딱맞아요 유성맛집 충남대맛집 헤이마오차이</td>\n",
       "      <td>\\n\\n\\n\\n\\n​요즘 코로나로 외식 해본지가 언제인지,,정말 오랜만에 약속잡고 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://blog.naver.com/3357266/222345771533</td>\n",
       "      <td>[충남대맛집 &amp; 궁동빵집 - 슈슈브레드]</td>\n",
       "      <td>\\n\\n\\n\\n\\n유명한 빵집이 정말 많은 대전!뭔가 대전은 빵의 도시인것 같다.​...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4745</th>\n",
       "      <td>https://blog.naver.com/mistakestory/222032617915</td>\n",
       "      <td>[진주맛집] 상대동 호동식당 _ 대구뽈찜</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n​​​​오빠의 강력 추천으...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4746</th>\n",
       "      <td>https://blog.naver.com/baiksange/221631319690</td>\n",
       "      <td>호동이식당 삼달리맛집</td>\n",
       "      <td>\\n\\n\\n\\n호동이식당 처음 가 봤는데 좋은걸요♡진주에서 강소농회원들과 함께 참석...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747</th>\n",
       "      <td>https://blog.naver.com/dlvjgy/221709966809</td>\n",
       "      <td>[인천/구월동] 연예인들도 많이 찾는 구월동 고기집 관교동 맛집 호동이갈비살에서 배...</td>\n",
       "      <td>\\n\\n\\n\\n​동동이가 며칠전부터갈비가 먹고싶다고 했었거든요월급받은 기념으로 갈비...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4748</th>\n",
       "      <td>https://blog.naver.com/jolee2020/222014740619</td>\n",
       "      <td>통영 복국 서호동 맛집 백년가게1호 통영 사람은 다 아는 호동 식당 시원함의 끝판왕...</td>\n",
       "      <td>\\n\\n\\n\\n\\n#통영맛집#통영백년가게#통영호동식당#통영복국#통영서호시장맛집#호동...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4749</th>\n",
       "      <td>https://blog.naver.com/elqlahs2/222074909812</td>\n",
       "      <td>순천 윗장 호동국밥(순천맛집)</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n순천에서는 어느 식당을 가...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4750 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   url                                              title                                               text\n",
       "0           https://blog.naver.com/fa1772/222377394130                                    취향저격 충남대 맛집 리스트  \\n\\n\\n\\n\\n최근 동창 친구와 약속이 있어서 충남대 맛집에 다녀왔는데요품질 좋...\n",
       "1          https://blog.naver.com/jhw7891/222038502625            혼밥하기 좋은 충남대 식당 5 / 충남대 맛집/ 충남대 혼밥/ 충대맛집  \\n\\n\\n\\n\\n길고 길었던 학기가 끝나고 이제 계절마저 끝으로 다가오고있다. ​...\n",
       "2         https://blog.naver.com/apfjtzzz/222354696565                        충남대 맛집 :: 삼각김밥의 클라쓰를 바꾼 강다짐  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n​남편은 예전부터 편의점 ...\n",
       "3            https://blog.naver.com/jmljw/222350051816                대전 마라탕 초보입에도 딱맞아요 유성맛집 충남대맛집 헤이마오차이  \\n\\n\\n\\n\\n​요즘 코로나로 외식 해본지가 언제인지,,정말 오랜만에 약속잡고 ...\n",
       "4          https://blog.naver.com/3357266/222345771533                             [충남대맛집 & 궁동빵집 - 슈슈브레드]  \\n\\n\\n\\n\\n유명한 빵집이 정말 많은 대전!뭔가 대전은 빵의 도시인것 같다.​...\n",
       "...                                                ...                                                ...                                                ...\n",
       "4745  https://blog.naver.com/mistakestory/222032617915                             [진주맛집] 상대동 호동식당 _ 대구뽈찜  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n​​​​오빠의 강력 추천으...\n",
       "4746     https://blog.naver.com/baiksange/221631319690                                        호동이식당 삼달리맛집  \\n\\n\\n\\n호동이식당 처음 가 봤는데 좋은걸요♡진주에서 강소농회원들과 함께 참석...\n",
       "4747        https://blog.naver.com/dlvjgy/221709966809  [인천/구월동] 연예인들도 많이 찾는 구월동 고기집 관교동 맛집 호동이갈비살에서 배...  \\n\\n\\n\\n​동동이가 며칠전부터갈비가 먹고싶다고 했었거든요월급받은 기념으로 갈비...\n",
       "4748     https://blog.naver.com/jolee2020/222014740619  통영 복국 서호동 맛집 백년가게1호 통영 사람은 다 아는 호동 식당 시원함의 끝판왕...  \\n\\n\\n\\n\\n#통영맛집#통영백년가게#통영호동식당#통영복국#통영서호시장맛집#호동...\n",
       "4749      https://blog.naver.com/elqlahs2/222074909812                                   순천 윗장 호동국밥(순천맛집)  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n순천에서는 어느 식당을 가...\n",
       "\n",
       "[4750 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "absolute_file_address = input(\"\")\n",
    "\n",
    "# read text data set\n",
    "text_set = pd.read_csv(absolute_file_address)\n",
    "text_set = text_set.fillna(\"\")\n",
    "text_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a447f9e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4750, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "751f12eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url      string\n",
       "title    object\n",
       "text     string\n",
       "dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_set = text_set.astype({'text':'string', 'url':'string'})\n",
    "text_set.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e983d35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_tmi_words_value: [ 5 13  4  4  7]\n",
      "own_words_value [265 251 148 211 249]\n",
      "commissional_words_value: [0 0 0 0 0]\n",
      "similar_sentence_value_all: [84 32 26 34 44]\n",
      "similar_sentence_value_bes:  [5 0 7 2 4]\n",
      "Analysis position: 0 -> 5\n",
      "\n",
      "\n",
      "use_tmi_words_value: [6 2 2 0 3]\n",
      "own_words_value [153 210 217 154 203]\n",
      "commissional_words_value: [0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "import itertools\n",
    "\n",
    "import copy\n",
    "import openpyxl\n",
    "import csv\n",
    "\n",
    "\n",
    "# co-occurrence matrix를 형성하는 함수\n",
    "def generate_co_occurrence_matrix(corpus):\n",
    "    vocab = set(corpus)\n",
    "    vocab = list(vocab)\n",
    "    vocab_index = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "    bi_grams = list(bigrams(corpus))\n",
    "    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n",
    "\n",
    "    co_occurrence_matrix = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "    for bigram in bigram_freq:\n",
    "        current = bigram[0][1]\n",
    "        previous = bigram[0][0]\n",
    "        count = bigram[1]\n",
    "        pos_current = vocab_index[current]\n",
    "        pos_previous = vocab_index[previous]\n",
    "        co_occurrence_matrix[pos_current][pos_previous] = count\n",
    "\n",
    "    co_occurrence_matrix = np.matrix(co_occurrence_matrix, dtype=np.int32)\n",
    "    return co_occurrence_matrix, vocab_index\n",
    "\n",
    "\n",
    "f = open('submission.csv', 'w', newline='')\n",
    "wr = csv.writer(f)\n",
    "# 과도한 정보 표기 , 고유 단어 사용, 고료 표기, 유사한 문장 사용, 유사한 문장 사용\n",
    "wr.writerow([\"url\", \"tmi\", \"use_unique_words\", \"indication_reward\", \"use_similar_sentences(including_body)\", \"use_similar_sentences(without_body)\"])\n",
    "\n",
    "# wb = openpyxl.load_workbook('submission.csv')\n",
    "# sheet = wb.active\n",
    "    \n",
    "# sheet.cell(row=1, column=1).value = \"과도한 정보 표기\"\n",
    "# sheet.cell(row=1, column=2).value = \"고유 단어 사용\"\n",
    "# sheet.cell(row=1, column=3).value = \"고료 표기\"\n",
    "# sheet.cell(row=1, column=4).value = \"유사한 문장 사용(본문 포함)\"\n",
    "# sheet.cell(row=1, column=5).value = \"유사한 문장 사용(본문 미포함)\"\n",
    "\n",
    "# wb.save(\"submission.csv\")\n",
    "\n",
    "analysis_count = 0\n",
    "while analysis_count < len(text_set):\n",
    "    if analysis_count+5 < len(text_set)-1:\n",
    "        text_sub_set = text_set[analysis_count : analysis_count+5]\n",
    "    else:\n",
    "        text_sub_set = text_set[analysis_count : len(text_set)-1]\n",
    "        \n",
    "    \n",
    "    # matrix(DataFrame)에서 text데이터를 가져온다.\n",
    "    text_list = text_sub_set['text']\n",
    "    url_list = text_sub_set['url']\n",
    "    url_list = url_list.tolist()\n",
    "    \n",
    "    # 한국어 조사 정리.\n",
    "    # 한국어 문법론 대가 이익섭 서울대 명예교수님께서 쓰신 '한국어문법'참고.\n",
    "    # 자연어처리 분야에서 한국어 조사는 분석의 까다로움으로 인해 전처리 때 아예 제거되는 불용어(stop_words)로 취급됨. \n",
    "    stop_words = [\"은\", \"는\", \"을\", \"를\", \"이\", \"가\", \"의\", \"에\", \"로\", \"으로\", \"과\", \"와\", \"도\", \"에서\", \"만\"\n",
    "                 , \"이나\", \"나\", \"까지\", \"부터\", \"에게\", \"보다\", \"께\", \"처럼\", \"이라도\", \"라도\", \"으로서\", \"로서\"\n",
    "                 , \"조차\", \"만큼\", \"같이\", \"마저\", \"이나마\", \"나마\", \"한테\", \"더러\", \"에게서\", \"한테서\", \"께서\"\n",
    "                 , \"이야\", \"이라야\"]\n",
    "\n",
    "    commissional_words = [\"협찬\", \"고료\", \"광고\", \"후원\", \"원고\"]\n",
    "\n",
    "    # Open Korea Text를 사용한 명사 추출 모듈 형성 - tokenizer진행\n",
    "    # 본래 CountVectorizer는 토크나이징과 벡터화를 동시해 해주나, 이는 한국어를 대상으로하지는 않음.\n",
    "    # 따라서 Okt모듈을 통해 토크나이징을 먼저 한 후 CountVectorizer작업 필요\n",
    "    okt=Okt()\n",
    "    text_token_set = list()\n",
    "\n",
    "    count = 0\n",
    "    for text in text_list.tolist():\n",
    "        text_token_set.append(okt.nouns(text))\n",
    "\n",
    "    # 시스템이 2층, 2층에, 2층으로 와 같은 글들을 각기 다른 하나의 명사 단어로 판단할수 있기에 토크나이징을 처리함\n",
    "\n",
    "\n",
    "    # 출력결과 NxM: N개의 데이터에서 M개의 데이터를 뽑아냄: 벡터화\n",
    "    # 결과적으로 각 문서에 어떤 단어가 몇번 등장했는지를 파악할 수 있음\n",
    "    try:\n",
    "        cv = CountVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
    "        tdm = cv.fit_transform(text_token_set)\n",
    "    except ValueError:\n",
    "#         dummy_value = np.zeros(shape=(len(text_list),), dtype=np.int64)\n",
    "        \n",
    "#         wb = openpyxl.load_workbook('submission.csv')\n",
    "#         sheet = wb.active\n",
    "\n",
    "#         for row in range(1, 6):\n",
    "#             sheet.cell(row=analysis_count+row+1, column=1).value = dummy_value[row-1]\n",
    "#             sheet.cell(row=analysis_count+row+1, column=2).value = dummy_value[row-1]\n",
    "#             sheet.cell(row=analysis_count+row+1, column=3).value = dummy_value[row-1]\n",
    "#             sheet.cell(row=analysis_count+row+1, column=4).value = dummy_value[row-1]\n",
    "#             sheet.cell(row=analysis_count+row+1, column=5).value = dummy_value[row-1]\n",
    "\n",
    "#         wb.save(\"submission.csv\")\n",
    "\n",
    "        for row in range(0, 5):\n",
    "            wr.writerow([0, 0, 0, 0, 0])\n",
    "\n",
    "        analysis_count = analysis_count+5\n",
    "\n",
    "\n",
    "    # TDM 출력\n",
    "    # TDM 대규모 데이터에서 대부분의 값은 0으로 나타낼 것임.\n",
    "    # 이유는 하나의 텍스트에 2000개 종류의 단어를 사용한다 해도, 전체 단어 셋은 몇 만개는 될 것이기에\n",
    "    # 메모리 부족 문제를 초래하기 쉽기 때문에, CountVectorizer는 희소행렬을 사용하기 때문.\n",
    "    # 다음 출력 결과는 희소행렬임\n",
    "    tdm_dataframe = pd.DataFrame(tdm.toarray())\n",
    "\n",
    "    # TDM 분석: 너무 독자적인 단어 사용, 너무 TMI적 단어 사용 감지 (평균 * 5회 보다 사용수가 많거나 같고, 5회 이상 사용된 단어)\n",
    "    use_tmi_words_value = np.zeros(shape=(len(text_list),), dtype=np.int64)\n",
    "    # TDM 분석: 모든 독자적인 단어에 대한 수치 (혼자 사용된 단어가 10개중 1개 미만 일 경우)\n",
    "    own_words_value = np.zeros(shape=(len(text_list),), dtype=np.int64)\n",
    "\n",
    "    for col in range(0, tdm_dataframe.shape[1]):\n",
    "        avg = sum(tdm_dataframe[col], 0.0) / len(tdm_dataframe[col]) # avg * 5 < 특정한 문서에서만 TMI적으로 사용된 단어 사용 빈도\n",
    "\n",
    "        for row in range(0, len(tdm_dataframe[col])):\n",
    "            if tdm_dataframe[col][row] >= (avg * 5) and tdm_dataframe[col][row] >= 5:\n",
    "                use_tmi_words_value[row] = use_tmi_words_value[row] + 1\n",
    "\n",
    "    print(\"use_tmi_words_value:\", use_tmi_words_value)\n",
    "\n",
    "\n",
    "    # TDM 분석: 모든 독자적인 단어에 대한 수치 (혼자 사용된 단어가 10개중 1개 미만 일 경우)\n",
    "    own_words_value = np.zeros(shape=(len(text_list),), dtype=np.int64)\n",
    "\n",
    "    for col in range(0, tdm_dataframe.shape[1]):\n",
    "        used_value = 0\n",
    "        for row in tdm_dataframe[col]:\n",
    "            if row != 0:\n",
    "                used_value += 1\n",
    "        for row in range(0, len(tdm_dataframe[col])):\n",
    "            if(len(text_list) == 0):\n",
    "                continue\n",
    "            \n",
    "            if len(text_list) >= 20:\n",
    "                if (used_value / 20) <= 1 and tdm_dataframe[col][row] != 0: \n",
    "                    own_words_value[row] = own_words_value[row] + 1\n",
    "\n",
    "            else:\n",
    "                if(used_value / len(text_list)) <= 1 and tdm_dataframe[col][row] != 0:\n",
    "                    own_words_value[row] = own_words_value[row] + 1\n",
    "\n",
    "\n",
    "    print(\"own_words_value\", own_words_value)\n",
    "\n",
    "\n",
    "    commissional_words_value = np.zeros(shape=(len(text_list),), dtype=np.int64)\n",
    "\n",
    "    for index in range(0, len(text_list)):\n",
    "        for word in commissional_words:\n",
    "            if(len(text_list) == 0):\n",
    "                continue\n",
    "            if(text_list.tolist()[index].find(word) != -1):\n",
    "                commissional_words_value[index] = 1\n",
    "\n",
    "    print(\"commissional_words_value:\", commissional_words_value)\n",
    "\n",
    "\n",
    "    # # Co-occurrence matrix 형성\n",
    "\n",
    "\n",
    "    co_occurrence_matrix_list = list()\n",
    "\n",
    "    for text in text_list:\n",
    "        tmp = text.split(\"\\n\")\n",
    "\n",
    "        text_data = [okt.nouns(line) for line in tmp]\n",
    "        text_data = list(itertools.chain.from_iterable(text_data))\n",
    "\n",
    "        matrix, vocab_index = generate_co_occurrence_matrix(text_data)\n",
    "        matrix_dataframe = pd.DataFrame(matrix, index=vocab_index, columns=vocab_index)\n",
    "        co_occurrence_matrix_list.append(matrix_dataframe)\n",
    "        \n",
    "\n",
    "    # Co-occurrence matrix 분석: 너무 독자적인 단어 사용, 너무 TMI적 단어 사용 감지\n",
    "    awkward_sentence_value = np.zeros(shape=(len(text_list),), dtype=np.int64)\n",
    "\n",
    "\n",
    "    # # 비슷한 문장 판독\n",
    "\n",
    "    # 1. 모든 문장에 대해 BoW적용\n",
    "    # 2. 모든 text 대이터에 대해 대해 유사글 수치 부여 (갱신)\n",
    "    # 3. 모든 text 데이터에 대해 유사글 수치 평균 계산\n",
    "\n",
    "\n",
    "    # text데이터 별 비슷한 문장 수 리스트\n",
    "    similar_sentence_value_all = np.zeros(shape=(len(text_list),), dtype=np.int64)\n",
    "    similar_sentence_value_bes = np.zeros(shape=(len(text_list),), dtype=np.int64)\n",
    "\n",
    "    # text_list토큰화\n",
    "    text_data_token_set = list()\n",
    "    for text in text_list:\n",
    "        lines = text.split('\\n')\n",
    "        lines_token_set = [okt.nouns(line) for line in lines]\n",
    "\n",
    "        text_data_token_set.append(lines_token_set)\n",
    "\n",
    "    comp_list = list(itertools.chain.from_iterable(text_data_token_set))\n",
    "\n",
    "    for i in range(0, len(text_list)):\n",
    "        for line in text_data_token_set[i]:\n",
    "            for comp in comp_list:\n",
    "                line_len = len(line)\n",
    "                comp_len = len(comp)\n",
    "                line_to_comp_sub_len = len([x for x in line if x not in comp])\n",
    "                comp_to_line_sub_len = len([x for x in comp if x not in line])\n",
    "\n",
    "                if line_len + comp_len != 0:\n",
    "                    if (line_to_comp_sub_len + comp_to_line_sub_len) / (line_len + comp_len) < 0.5:\n",
    "                        similar_sentence_value_all[i] = similar_sentence_value_all[i] + 1\n",
    "\n",
    "    for i in range(0, len(text_list)):\n",
    "        for line in text_data_token_set[i]:\n",
    "\n",
    "            explore_dest = copy.deepcopy(text_data_token_set)\n",
    "            del explore_dest[i]\n",
    "            for comp in list(itertools.chain.from_iterable(explore_dest)):\n",
    "                line_len = len(line)\n",
    "                comp_len = len(comp)\n",
    "                line_to_comp_sub_len = len([x for x in line if x not in comp])\n",
    "                comp_to_line_sub_len = len([x for x in comp if x not in line])\n",
    "\n",
    "                if line_len + comp_len != 0:\n",
    "                    if (line_to_comp_sub_len + comp_to_line_sub_len) / (line_len + comp_len) < 0.5:\n",
    "                        similar_sentence_value_bes[i] = similar_sentence_value_bes[i] + 1\n",
    "\n",
    "    print(\"similar_sentence_value_all:\", similar_sentence_value_all)\n",
    "    print(\"similar_sentence_value_bes: \", similar_sentence_value_bes)\n",
    "    print(\"Analysis position:\", analysis_count, \"->\", analysis_count+5)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    \n",
    "#     wb = openpyxl.load_workbook('submission.csv')\n",
    "#     sheet = wb.active\n",
    "\n",
    "#     for row in range(1, 6):\n",
    "#         if(len(use_tmi_words_value) < row):\n",
    "#             break\n",
    "#         sheet.cell(row=analysis_count+row+1, column=1).value = use_tmi_words_value[row-1]\n",
    "#         sheet.cell(row=analysis_count+row+1, column=2).value = own_words_value[row-1]\n",
    "#         sheet.cell(row=analysis_count+row+1, column=3).value = commissional_words_value[row-1]\n",
    "#         sheet.cell(row=analysis_count+row+1, column=4).value = similar_sentence_value_all[row-1]\n",
    "#         sheet.cell(row=analysis_count+row+1, column=5).value = similar_sentence_value_bes[row-1]\n",
    "        \n",
    "#     wb.save(\"submission.csv\")\n",
    "\n",
    "\n",
    "    for row in range(0, 5):\n",
    "        if(len(use_tmi_words_value) <= row):\n",
    "            break\n",
    "        wr.writerow([url_list[row], use_tmi_words_value[row], own_words_value[row], commissional_words_value[row], similar_sentence_value_all[row], similar_sentence_value_bes[row]])\n",
    "        \n",
    "        \n",
    "    analysis_count = analysis_count+5\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464ddf71",
   "metadata": {},
   "source": [
    "# 결과 기록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a56ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"text_module_finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5c2365",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
